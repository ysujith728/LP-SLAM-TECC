<h1 align="center">ğŸš€ LP-SLAM-TECC</h1>

<h3 align="center">
Language-Perception SLAM with <br>
Text Error Correction (TECC) + RAG + LLM
</h3>

<p align="center">
<b>Semantic SLAM system that understands text in the environment</b>
</p>

<p align="center">
<a href="https://github.com/ysujith728/LP-SLAM-TECC">GitHub Repository</a>
</p>

<hr>

<h2>ğŸ“Œ Abstract</h2>

<p>
Traditional SLAM systems focus primarily on geometry and ignore semantic
information present in the environment. <b>LP-SLAM-TECC</b> enhances SLAM by
integrating scene text understanding into the mapping process.
</p>

<p>
The system detects text from images, corrects OCR errors using
<b>Text Error Correction & Classification (TECC)</b>, enriches understanding
using <b>Retrieval-Augmented Generation (RAG)</b>, and stores results in a
semantic map. This enables robots to understand signs like <i>EXIT</i>,
room numbers, and labels in a human-like way.
</p>

<hr>

<h2>ğŸ§  System Architecture</h2>

<pre>
Camera / Image
      â†“
OCR (EasyOCR)
      â†“
TECC (Correction + Classification)
      â†“
RAG (FAISS + Knowledge Base)
      â†“
Semantic Map
</pre>

<hr>

<h2>âœ¨ Key Features</h2>

<ul>
  <li>ğŸ” Robust OCR using EasyOCR</li>
  <li>ğŸ›  Text Error Correction (handles OCR noise)</li>
  <li>ğŸ§  Semantic Classification using LLM-ready TECC</li>
  <li>ğŸ“š Retrieval-Augmented Generation (RAG)</li>
  <li>ğŸ—º In-memory Semantic Map for SLAM</li>
  <li>ğŸŒ Interactive Web UI (FastAPI + HTML/CSS/JS)</li>
</ul>

<hr>

<h2>ğŸ“‚ Project Structure</h2>

<pre>
LP-SLAM-TECC/
â”œâ”€â”€ app.py                  # FastAPI backend
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ knowledge_base/
â”‚   â””â”€â”€ rag_data.jsonl
â”œâ”€â”€ slam_engine/
â”‚   â”œâ”€â”€ text_detection.py
â”‚   â”œâ”€â”€ tecc_model.py
â”‚   â”œâ”€â”€ rag_engine.py
â”‚   â””â”€â”€ tecc_db.json
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ ui.css
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ build_rag_index.py
â”œâ”€â”€ rag_index.faiss
â”œâ”€â”€ test_pipeline.py
â””â”€â”€ README.md
</pre>

<hr>

<h2>âš™ï¸ Installation & Setup</h2>

<h3>1ï¸âƒ£ Clone the Repository</h3>

<pre>
git clone https://github.com/ysujith728/LP-SLAM-TECC.git
cd LP-SLAM-TECC
</pre>

<h3>2ï¸âƒ£ Create Virtual Environment</h3>

<pre>
python -m venv venv
venv\Scripts\activate
</pre>

<h3>3ï¸âƒ£ Install Dependencies</h3>

<pre>
pip install -r requirements.txt
</pre>

<hr>

<h2>ğŸš€ Running the Project</h2>

<pre>
uvicorn app:app --reload
</pre>

<p>
Open your browser and go to:
<b>http://127.0.0.1:8000</b>
</p>

<hr>

<h2>ğŸ§ª Example Output</h2>

<p>
Input image containing text:
</p>

<pre>
EXIT
</pre>

<p>
Semantic Map Output:
</p>

<pre>
{
  "text": "EXIT",
  "label": "exit_sign",
  "meaning": "Indicates an emergency exit used for evacuation."
}
</pre>

<hr>

<h2>ğŸ—º Semantic Mapping</h2>

<p>
The semantic map stores:
</p>

<ul>
  <li>Detected text</li>
  <li>Corrected text</li>
  <li>Semantic label</li>
  <li>Contextual meaning</li>
</ul>

<p>
This allows the SLAM system to reason about the environment instead of
just mapping walls.
</p>

<hr>

<h2>ğŸ¯ Applications</h2>

<ul>
  <li>ğŸ¤– Indoor robot navigation</li>
  <li>ğŸ¢ Smart buildings</li>
  <li>â™¿ Assistive navigation systems</li>
  <li>ğŸ“ Human-aware SLAM research</li>
</ul>

<hr>

<h2>ğŸ”¬ Research Contribution</h2>

<ul>
  <li>Introduces text as a first-class SLAM landmark</li>
  <li>Demonstrates TECC + RAG synergy</li>
  <li>Reduces OCR misclassification errors</li>
  <li>Bridges vision, language, and mapping</li>
</ul>

<hr>

<h2>ğŸ‘¨â€ğŸ’» Author</h2>

<p>
<b>Sujith Y</b><br>
B.Tech CSE<br>
GitHub: <a href="https://github.com/ysujith728">ysujith728</a>
</p>

<hr>

<h2>ğŸ“œ License</h2>

<p>
This project is intended for academic and research purposes.
</p>

<hr>

<p align="center">
â­ If you like this project, consider starring the repository â­
</p>
